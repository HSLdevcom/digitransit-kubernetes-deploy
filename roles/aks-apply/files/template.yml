apiVersion: v1
kind: Service
metadata:
  name: <NAME>
spec:
  ports:
  - port: <PORT>
    targetPort: <targetPORT>
    name: <NAME>-service-port
    protocol: TCP
  selector:
    app: <NAME>
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: <NAME>
  labels:
    app: <NAME>
spec:
  replicas: <REPLICAS_AMOUNT>
  strategy:
    type: RollingUpdate
    rollingUpdate: # Depends on number of replicas and if it is prod or dev, prod should always have 0 maxUnavaiblable, in dev there should always be at least on replica running
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: <NAME>
  template:
    metadata:
      labels:
        app: <NAME>
        lastRestartDate: dummy-value #needed for deployer
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: Exists
              topologyKey: kubernetes.io/hostname
      nodeSelector:
        pool: <POOL_NAME>
      containers:
      - name: <NAME>
        image: hsldevcom/<IMAGE>:latest
        volumeMounts:
          - mountPath: </path/>
            name: <MOUNT_NAME>
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
        ports:
        - containerPort: <PORT>
        readinessProbe: # Some services that dont have external endpoints don't require probes but otherwise readinessProbe should be used
          periodSeconds: <should be slightly less than intervalSeconds>
          timeoutSeconds: <timeoutSeconds>
          failureThreshold: <periodSeconds * maxConsecutiveFailures == gracePeriodSeconds>
          httpGet: # In theory, this could be different than livenessProbe's httpGet, for example, pod could be taken off load balancer during some maintenance process
            port: <PORT>
            path: </path/>
        livenessProbe: # If service can't get stuck or considerably slow down, livenessprobe is not necessary and could even be harmful
          initialDelaySeconds: <gracePeriodSeconds>
          periodSeconds: <intervalSeconds>
          timeoutSeconds: <timeoutSeconds>
          failureThreshold: <maxConsecutiveFailures>
          httpGet:
            port: <PORT>
            path: </path/>
        env:
        - name: ENV_NAME
          value: "VALUE"
        resources:
          requests:
            memory: <mem>
          limits:
            memory: <mem>
            cpu: <cpus>
      imagePullSecrets:
        - name: hsldevcomkey      
      volumes:
        - name: <MOUNT_NAME>
          emptyDir: {}

---

apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: <NAME>-pdb
spec:
  minAvailable: 50% #no disruption budget if 1 replica, 50% if 2-3, 75% for >3
  selector:
    matchLabels:
      app: <NAME>
